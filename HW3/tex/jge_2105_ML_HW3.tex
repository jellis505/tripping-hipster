%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{bm}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{graphicx} % This one is for pictures
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{color}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
%\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
%\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Columbia University -- Fall 2013} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Machine Learning Homework \#3\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Joe Ellis - jge2105} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{Problem 1}
\subsection{Problem1.1 - Discrete}
We assume that we have a coin that can land on heads or tails, and we have 3 possible values for $\mu$, which are $\frac{1}{2},\frac{1}{4},\frac{3}{4}$. 
 The first problem that we address is to find the minimum number of tosses we'd need to see in order to conclude that $p(\mu = \frac{1}{2}) > \frac{1}{2}$.
We will refer to this series of tosses as, $D$.  Using Bayes rule we know that $p(\mu|D) = p(D|\mu)p(\mu)/p(D)$.  
For this example, we can assume that the prior on each choice of mu is equal.  We know that the flipping of a coin is given by the bernoulli distribution, where$ N_{H}$ and $N_{T}$ are the number of heads and tails in our set respectively.
We will ignore $p(\mu)$ in the equations since the priors are uniform.
The quickest time that we can be sure that the $p(\mu=\frac{1}{2}|D) > \frac{1}{2}$ is $D = (H,T,H,T,H,T)$

\begin{align}
p(\mu=\frac{1}{2}|D) &= \frac{(\mu)^{N_{H}}(1-\mu)^{N_{T}}}{\sum\limits_{\mu} (\mu)^{N_{H}}(1-\mu)^{N_{T}}  } \\
&= .5424
\end{align}
 
The second test that we will do on this is to find the minimal set $D$ such that $p(\mu = \frac{3}{4} | D) > \frac{1}{2}$. 
 For this the smallest dataset that makes this a reality is $D = (H,H)$, by the same logic.

\begin{align}
p(\mu=\frac{3}{4}|D) &= \frac{(\mu)^{N_{H}}(1-\mu)^{N_{T}}}{\sum\limits_{\mu} (\mu)^{N_{H}}(1-\mu)^{N_{T}}  } \\
&= .6429
\end{align}

\subsection{Problem 1.2 - Continuous}
 Consider 2 possible distributions: (A) $\mu ~ uniform[0,1]$; (B) we have some reason to think the coin is likely to be fair, so we have a parabola that is peaked at $\frac{1}{2}$ and then goes to 0 at 0 and 1.

Use the two datasets, and answer a variety of questions for each, $D_1 = (H,T)$; $D_2 = (T,T,T)$.

We will use the following arguments with the beta distribution.

\begin{align}
Beta(\mu,a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}(1-\mu)^{a-1}\mu^{b-1} \\
E(Beta(\mu) ) = \frac{a}{a+b} \\
Var(Beta(\mu) ) = \frac{ab}{(a+b)^2(a+b+1)} \\
\Gamma(n) = (n-1)!
\end{align}


\subsubsection{A -- 1}
\paragraph{P(H) given the prior}
The probability of $p(H) = \int_{\mu} p(H|\mu)p(\mu)$, and since $\mu$ is uniform then we know that the $p(\mu) =1$, because the area under the prior must integrate to 1 to be a true probability distribution.  Therefore, we have

\begin{align}
p(H) &= \int_{\mu} p(H|\mu)p(\mu) \\
p(H) &= \int_{\mu} p(H|\mu) \\
p(H) &= \frac{\mu^2}{2} |_{0}^{1} \\
p(H) &= \frac{1}{2}.
\end{align}

\paragraph{$p(\mu|D)$}
This is the posterior distribution given the dataset $D$.  We know that the $Beta(\mu,1,1) = uniform distribution$.

\begin{align}
p(\mu|D) &= \frac{p(D|\mu)p(\mu)}{p(D)} \\
&= \frac{p(D|\mu)}{p(D)} \\
&= Beta(\mu,2,2) \\
&= \frac{3!}{1!1!}\mu(1-\mu) \\
& = 6 \mu(1-\mu)
\end{align}

\paragraph{$\mu_{ML}$ given $D$}
The maximum likelihood estimate of this value is the peak of the function above, which is at $\mu = \frac{1}{2}$.
This has been proven in class from the lectures, that the ML solution is  $\mu = \frac{N_H}{N_H+N_T}$, where $N_H$ is the number of heads and $N_T$ is the number of tails.
 
\paragraph{$\mu_{MAP}$ given $D$}
We know that $\mu_{MAP} = argmax p(\mu|D)p(\mu)$, but since our prior is uniform here $\mu_{MAP} = \mu_{ML}$.  Therefore, $\mu_{MAP} = \frac{1}{2}$.

\paragraph{$p(H|D)$}
Now let's find the full bayesian integral for the value given the found $\mu$, that we have for our dataset.  Also, remember for our problem $p(H|\mu) = \mu)$, because $\mu$ is the probability that for any given toss we get heads.

\begin{align}
p(H|D) &= \int_u p(H|\mu)p(\mu|D)d\mu \\
&= \int_u \mu p(\mu|D)d\mu \\
&= E( p(\mu|D)) \\
&= E( Beta(\mu,2,2)) \\
&= \frac{2}{2+2} \\
&= \frac{1}{2} \\
\end{align}

\paragraph{$Var(p(\mu|D))$}

\begin{align}
Var(p(\mu|D)) &=  Var(Beta(\mu,2,2)) \\
&= \frac{2*2}{(2+2)^2(2+2+1)} \\
&= 0.05 
\end{align}

\subsubsection{A -- 2}
\paragraph{P(H) given the prior}
Same as seen in A-1, because the prior did not change.

\paragraph{$p(\mu|D)$}
This is the posterior distribution given the dataset $D$. 

\begin{align}
p(\mu|D) &= \frac{p(D|\mu)p(\mu)}{p(D)} \\
&= \frac{p(D|\mu)}{p(D)} \\
&= Beta(\mu,1,4) \\
&= \frac{4!}{0!3!}(1-\mu)^3 \\
& = 4 (1-\mu)^3
\end{align}

\paragraph{$\mu_{ML}$ given $D$}
The maximum likelihood estimate of this value is the peak of the function above, which is at $\mu = 0$.
This has been proven in class from the lectures, that the ML solution is  $\mu = \frac{N_H}{N_H+N_T}$, where $N_H$ is the number of heads and $N_T$ is the number of tails, and we did not see any heads.

\paragraph{$\mu_{MAP}$ given $D$}
We know that $\mu_{MAP} = argmax p(\mu|D)p(\mu)$, but since our prior is uniform here $\mu_{MAP} = \mu_{ML}$.  Therefore, $\mu_{MAP} = 0$.

\paragraph{$p(H|D)$}
Now let's find the full bayesian integral for the value given the found $\mu$, that we have for our dataset.  Also, remember for our problem $p(H|\mu) = \mu)$, because $\mu$ is the probability that for any given toss we get heads.

\begin{align}
p(H|D) &= \int_u p(H|\mu)p(\mu|D)d\mu \\
&= \int_u \mu p(\mu|D)d\mu \\
&= E( p(\mu|D)) \\
&= E( Beta(\mu,1,4)) \\
&= \frac{1}{1+4} \\
&= \frac{1}{5} \\
\end{align}

\paragraph{$Var(p(\mu|D))$}

\begin{align}
Var(p(\mu|D)) &=  Var(Beta(\mu,1,4)) \\
&= \frac{1*4}{(5)^2(1+4+1)} \\
&= 0.0267
\end{align}

\subsubsection{B--1}
\paragraph{P(H) given the prior}
The probability of $p(H) = \int_{\mu} p(H|\mu)p(\mu)$, and $p(\mu)$ is not uniform, which means that we need to include it in the integral.  We will use the function for $p(\mu) =  Beta(\mu,2,2)$. Therefore, 

\begin{align}
p(H) &= \int_{\mu} p(H|\mu)p(\mu) \\
P(H) &= \frac{3!}{1!1!}\int_{\mu} \mu*(\mu)(1-\mu) \\
p(H) &=  \frac{3!}{1!1!} \int_{\mu} \mu^2-\mu^3 \\
P(H) &= 6 \frac{(x^3)}{3} - \frac{(x^4)}{4} |^1_0 \\
P(H) &= 6 (\frac{1}{12}) \\
P(H) &= \frac{1}{2}. \\
\end{align}

\paragraph{$p(\mu|D)$}
This is the posterior distribution given the dataset $D$.

\begin{align}
p(\mu|D) &= \frac{p(D|\mu)p(\mu)}{p(D)} \\
&= \frac{p(D|\mu)}{p(D)} \\
&= Beta(\mu,3,3) \\
&= \frac{5!}{2!2!}(\mu)^2(1-\mu)^2 \\
& = 30 \mu^2(1-\mu)^2
\end{align}

\paragraph{$\mu_{ML}$ given $D$}
This portion of the function is the same as A-1, because ML assumes we have uniform priors.

\paragraph{$\mu_{MAP}$ given $D$}
For this more complicated function we have not seen in class, let's take the derivative and set equal to zero.

\begin{align}
\frac{\partial}{\partial \mu} 30 \mu^2(1-\mu)^2 &= 0 \\
\frac{\partial}{\partial \mu} \mu^2 (1 -2\mu + \mu^2) &= 0 \\
\frac{\partial}{\partial \mu} \mu^2 -2 \mu^3 + \mu^4 &= 0 \\
2\mu -6\mu^2 + 4\mu^3 &= 0 \\
\frac{1}{2} &= \mu \\
\end{align}

\paragraph{$p(H|D)$}
Now let's find the full bayesian integral for the value given the found $\mu$, that we have for our dataset.  Also, remember for our problem $p(H|\mu) = \mu)$, because $\mu$ is the probability that for any given toss we get heads.

\begin{align}
p(H|D) &= \int_u p(H|\mu)p(\mu|D)d\mu \\
&= \int_u \mu p(\mu|D)d\mu \\
&= E( p(\mu|D)) \\
&= E( Beta(\mu,3,3)) \\
&= \frac{3}{3+3} \\
&= \frac{1}{2} \\
\end{align}

\paragraph{$Var(p(\mu|D))$}

\begin{align}
Var(p(\mu|D)) &=  Var(Beta(\mu,3,3)) \\
&= \frac{3*3}{(6)^2(3+3+1)} \\
&= .0357
\end{align}

\subsection{B -- 2}
\paragraph{P(H) given the prior}
See the same as B-1.

\paragraph{$p(\mu|D)$}
This is the posterior distribution given the dataset $D$.

\begin{align}
p(\mu|D) &= \frac{p(D|\mu)p(\mu)}{p(D)} \\
&= \frac{p(D|\mu)}{p(D)} \\
&= Beta(\mu,2,5) \\
&= \frac{6!}{1!5!}(\mu)^1(1-\mu)^5 \\
& = 6 \mu(1-\mu)^5
\end{align}

\paragraph{$\mu_{ML}$ given $D$}
For this answer please see A-2, because ML assumes that we have a uniform prior.

\paragraph{$\mu_{MAP}$ given $D$}
For this more complicated function we have not seen in class, let's take the derivative and set equal to zero.

\begin{align}
\frac{\partial}{\partial \mu} 6 \mu(1-\mu)^5 &= 0 \\
 -5*\mu(1-\mu)^4 + (1-\mu)^5 &= 0 \\
-5\mu + (1-mu )&= 0 \\
 1 &= 6\mu \\
\mu &= \frac{1}{6}
\end{align}

\paragraph{$p(H|D)$}
Now let's find the full bayesian integral for the value given the found $\mu$, that we have for our dataset.  Also, remember for our problem $p(H|\mu) = \mu)$, because $\mu$ is the probability that for any given toss we get heads.

\begin{align}
p(H|D) &= \int_u p(H|\mu)p(\mu|D)d\mu \\
&= \int_u \mu p(\mu|D)d\mu \\
&= E( p(\mu|D)) \\
&= E( Beta(\mu,2,5)) \\
&= \frac{2}{2+5} \\
&= \frac{2}{7} \\
\end{align}

\paragraph{$Var(p(\mu|D))$}

\begin{align}
Var(p(\mu|D)) &=  Var(Beta(\mu,2,5)) \\
&= \frac{2*5}{(7)^2(2+5+1)} \\
&= .0255
\end{align}


%----------------------------------------------------------------------------------------
%	PROBLEM 2
%----------------------------------------------------------------------------------------
\section{Problem 2}
We are assuming that we have a good test for swine flu that is highly accurate, with the probabilities given below.  
We want to find the likelihood if I (Joe) take the test and it outputs true, what is the probability that I have swine flu.

\begin{align}
p(test=true|flu=True) &= 0.99 \\
p(test=false|flu=false) &= 0.98 \\
p(flu=true) &= .0001 \\
p(flu=false) &= .999
\end{align}

Let's use Bayes Rule to solve this problem.

\begin{align}
p(flu=true|test=True) &= \frac{p(test=true|flu=True) p(flu=true)}{p(test=true)} \\
%&=  \frac{p(test=true|flu=True) p(flu=true)}{p(test=true|flu=True) p(flu=true) + p(test=true|flu=False) p(flu=false)} \\
&= \frac{0.99*0.0001}{0.99*0.0001+(1-0.98)*0.999} \\
&= 0.0049 \\
\end{align}


So still not that likely, shows the power of the prior.

%----------------------------------------------------------------------------------------
%	PROBLEM 3
%----------------------------------------------------------------------------------------
\section{Problem 3}

\subsection{Problem A}
We want to show that $u \perp (v,w)|x \Rightarrow u \perp v|x$.

\paragraph{PROOF}
Was not able to solve this proof.  

\subsection{Problem B}
We want to show that $A \perp (B,C) \Rightarrow A \perp B$.

\paragraph{PROOF}

\begin{align}
p(A,B,C) &= p(A)p(B,C) \\
\int_C  p(A,B,C) &= \int_C p(A)p(B,C) \\
p(A,B) &= p(A)p(B) \\
A &\perp B 
\end{align}


%----------------------------------------------------------------------------------------
%	PROBLEM 4
%----------------------------------------------------------------------------------------
\section{Problem 4}
The weibull distribution is a probablility distribution over non-negative scalar values.  
The distribution is as follows,  $p(x|\lambda) = \frac{3}{\lambda}(\frac{x}{\lambda})^2exp(-(\frac{x}{\lambda})^3)$.
Given a dataset $D = (x_0,x_1,...,x_N)$, what is the ML estimate of $\lambda$.
Let's use the log-likelihood.

\begin{align}
l(D|\lambda) &= \sum log(p(x_i|\lambda) \\
&= \sum log(\frac{3}{\lambda})+2log(\frac{x_i}{\lambda})-(\frac{x_i}{\lambda})^3 \\
&= \sum log(3) - log(\lambda) +2log(x_i) - 2log(\lambda) -(\frac{x_i}{\lambda})^3 
\end{align}

Now let's take the derivative of the log-likelihood, and set to 0.

\begin{align}
\frac{\partial l}{\partial \lambda} &=\frac{\partial}{\partial \lambda} \sum log(3) - log(\lambda) +2log(x_i) - 2log(\lambda) -(\frac{x_i}{\lambda})^3 \\
\sum -\frac{1}{\lambda} - \frac{2}{\lambda}  + 3\lambda^{-4}x_i^{3} &= 0 \\
\frac{-3N}{\lambda} + \sum 3\lambda^{-4}x_i^{3} &= 0 \\
 \frac{3N}{\lambda} &=  \sum 3\lambda^{-4}x_i^{3} \\ 
\frac{3N\lambda^4}{3\lambda} &= \lambda^4 \sum 3\lambda^{-4}x_i^{3} \\
\lambda^3 &= \sum \frac{x_i^{3}}{N} \\
\lambda &= (\sum \frac{x_i^{3}}{N})^{1/3}
\end{align}

\section{Problem 5}
In problem 5 we create a kernelized logistic regression function using the RBF, kernel.
We optimize the loss function that can be seen below using stochastic gradient descent, basic gradient descent using these methods. The optimization function can be seen below,

\begin{equation}
J(w) = \sum\limits_{i=1}^N log(\sigma(y_i w^T k_i)) + \lambda w^T w.
\end{equation}

We then take the gradient, described here.
\begin{equation}
\frac{\partial J(w)}{\partial w} = \sum\limits_{i=1}^M -y_i  k_i log(\sigma(-y_i w^T k_i)) + \frac{\lambda}{2}  w
\end{equation}

Using this update step, we end up with the update rule here,

\begin{equation}
w^{t} = w^{t-1} + \eta (  \sum\limits_{i=1}^M -y_i  k_i log(\sigma(-y_i w^T k_i)) + \lambda w).
\end{equation}



We remove the 2 from the gradient without the loss of generality.
Finally, we select $\eta = .001$, and the Regularizer Penalty as $\lambda = .01$.
We were able to get a training accuracy of 91.16\%, and test accuracy 79.503\% of  using mini-batch gradient descent with 100 points.  
Using full gradient descent, which is the same as batch gradient descent, just using the entire batch for the descent we were able to achieve training accuracy of  80.1\%, and testing accuracy of 60.52\%.
This worse results on the full gradient descent, is due to an iteration limit that I set on the descent due to the fact that it was taking to long to converge.  
Therefore, the stochastic gradient descent method worked much better for our purposes and converged much quicker.
The stochastic method achieves convergence much quicker, and we used randomly chosen points for each update step.
All of the code that was used to create can be found in the zip file attached to the write up.
Also, my memory was large enough to read the entire Kernel Gram matrix into memory, and because it was much quicker to do so I calculated the gram matrix at the beginning.
A plot of the decreasing cost function during the stochastic gradient descent algorithm can be seen in Figure \ref{fig:cost}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{cost.png}
\caption{Cost Function during Stochastic Gradient Descent}
\label{fig:cost}
\end{figure}

\end{document}