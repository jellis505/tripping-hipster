%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{bm}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{graphicx} % This one is for pictures
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{color}
\usepackage{float}
\floatplacement{figure}{H}
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
%\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
%\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Columbia University -- Fall 2013} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Machine Learning Homework \#5\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Joe Ellis - jge2105} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{Problem 1}
Assume you are a contestant of a game show in which you are presented with three closed doors A, B, and C. 
Behind one of the doors is a car which will be yours if you choose the right door. 
After you have randomly (as you have no prior information) selected a door (say door A), the game host opens door B which has nothing inside, while keeping door A and C closed. 
The host then asks whether you want to change your selection from A to C. 
Should you change?

We should change, and the following rational should describe why.
Let's assume WLOG that we choose a door, and model our choice based on the random variable $X$.  
The door with the actual prize behind it will be modeled by the random variable $W$, and the door chosen by the host of the show will be model another random variable $Y$.
Let's try to make a graphical model based on these random variables.  
First, we know that $X$ is not directly dependent on $W$, because I have no possible way to know the state of $W$.
Therefore, in a directed graphical model, we know that there is no connection between these two nodes.
However, $Y$ (the door that the host chooses) is directly effected by the value of $X$ (the door I choose), and $W$, the actual winning door. 
Therefore, we have a unidirected model with connections from $X$ to $Y$ and $W$ to $Y$.

So our first choice of a door or $X$ is made with no knowledge of $Y$, therefore based on slide 12 of lecture 15, because $Y$ is unseen. 
We have $X \perp W$.
Which means that our choice of $X$ is completely independent of the actual winning door, and therefore each door has an equal shot of holding the winning prize.  
Since there are 3 doors, and we choose each equally and each has an equal shot of containing the prize the likely hood of our choice $X$ resulting in the prize is $\frac{1}{3}$.

However, after we have seen the value of $Y$, $X$ and $W$ are no longer independent.
So we want to find the probability function for $W$ given that we have observed $X$ and $Y$.
Therefore, let's do this using Bayes' Rule, and use the fact that $X \perp W$, before we see $Y$ in the derivation below.

\begin{align}
P(W|X,Y) &= \frac{P(X,Y|W)}{\sum_W P(X,Y|W)} \\
&= \frac{P(Y|X,W)P(X|W)}{\sum_W P(Y|X,W)P(X|W)} \\
&= \frac{P(Y|X,W)P(X)}{\sum_W P(Y|X,W)P(X)} \\
&= \frac{P(X)P(Y|X,W)}{P(X) \sum_W P(Y|X,W)} \\
&= \frac{P(Y|X,W)}{ \sum_W P(Y|X,W)} 
\end{align}

Now that we have these probabilities let's evaluate the scenario described.
We chose A, and therefore the game show host has opened door B and nothing is inside.
Let's find the probabilities that the winning prize is in our door A or if we should switch to door C.
We will not normalize the probabilities below, because we assume as described earlier that the probabilities of choosing any of the doors is equal, and the probability of having the winner behind any door is also equal.  Therefore, the priors do not matter

\begin{align}
p(W=A|X=A, Y=B) &= \frac{P(Y=B|X=A,W=A)}{ \sum_W P(Y=B|X=A,W)} \\
&= \frac{0.5}{0.5+0+1}  \\
&= \frac{1}{3}.
\end{align}

\begin{align}
p(W=A|X=A, Y=B) &=\frac{P(Y=B|X=A,W=C)}{ \sum_W P(Y=B|X=A,W)} \\
&= \frac{1}{0.5+0+1}  \\
&= \frac{2}{3}.
\end{align}

Therefore, if we switch the door that we choose to C, we would have a $\frac{2}{3}$ chance of winning!  
If we stay we only have a $\frac{1}{3}$ chance of winning.
So we should switch!

%----------------------------------------------------------------------------------------
%	PROBLEM 2
%----------------------------------------------------------------------------------------

\section{Problem 2}
\subsection{Problem 2a}
We want to prove that $X \perp Y|Z$ if and only if the joint probability $p(x,y,z)$ can be expressed in the form of $a(x,z)b(y,z)$.
\paragraph{PROOF}
Assume that $X \perp Y|Z$, therefore we know that $p(x|y,z) = p(x,z)$ and $p(x|y) \neq p(x)$.

\begin{align}
p(x,y,z) &= p(y,z)p(x|y,z) \\
&= p(y,z)p(x,z).
\end{align}

Therefore, we have $p(x,y,z)$ can be expressed in the form of $a(x,z)b(y,z)$.

\subsection{Problem 2b}
 We want to prove or disprove with a counter example, that $X \perp Y|Z$ and $X \perp W|(Y,Z)$ implies that $X \perp (W,Y)|Z$.
We want to use this knowledge to prove that $p(x|(w,y,z)) = p(x|z)$.

\paragraph{PROOF}
\begin{align}
p(x|(w,y,z)) &= p(x|y,z),\ by \ X \perp W|(Y,Z) \\
&= p(x|z), \ by \  X \perp Y|Z.
\end{align}

Thus, we have shown $X \perp (W,Y)|Z$.

\subsection{Problem 2c}
We want to prove that $X \perp Y|(Z,W)$ and $X \perp Z|(Y,W)$ implies that $X \perp (Y,Z)|W$.  
We know from $X \perp Y|(Z,W)$ that the $p(X|Y,Z,W) = p(X|Z,W)$, and from $X \perp Z|(Y,W)$ we know $p(X|Y,Z,W) = p(X|Y,W)$.
This also in turn implies that $p(X|Y,W) = p(X|Z,W)$.

We want to show that $p(X|Y,Z,W) = p(X|W)$.

\begin{align}
p(X|Z,Y,W) &= \\
\end{align}


\subsection{Bonus}
Given a directed a cyclic graphical model composed of nodes $X_1,X_2,X_3,...,X_n$, we want to prove that the conditional independence statements $X_i \perp X_{non-desc of i}|X_{\pi i}$ implies that the joint distirubtion can be decomposed as follows:

\begin{align}
P(X_1,X_2,X_3,...,X_n) &= \prod_{i=1}^n P(X_i|X_{\pi i}) \\
\end{align}

Ok, first let's rationalize what not having any cycles in the graphs means.  This means, that if we are able to start a particular parent node and label this node 1, that all other nodes will be children of this node.
Because the graph is acyclic no nodes that are children of a particular node can loop back on a path and in turn become parents of said node.  
Therefore, let's rename the nodes making the derivation of the problem easier.
Start at the root of the graphical model and name this node $Y_i$, and recursively travel down to this node's descendants label them $Y_2,Y_3,..,Y_m$, where $m$ is the last node labeled that is a direct descendant of $Y_1$.  
Then perform this algorithm recursively for each node making sure that each node has a new and unique identifier, and incrementing by one by the last existing identifier.
For the second node this algorithm is performed on it's first chosen direct descendant is $Y_{m+1}$.

This gives us a graph where for $i > j$, in the same path we know that $Y_i$ is a descendant of $Y_j$.  
Although they may not be a direct descendant.
If they are not in the same path the ordering of these nodes does not matter.
The above statements hold due to the acyclical nature of our graphical model.
Now let's prove the theorem above.

\begin{align}
P(X_1,...,X_n) &= P(Y_1,...,Y_N), \ by\ same \ elements, \\
&= P(Y_2,...,Y_n|Y_1)P(Y_1) \\
&=... \\
&= P(Y_n|Y_{n-1},...,Y_1)...P(Y_2|Y_1)P(Y_1) \\
&= \prod_i^n P(Y_i|Y_{non-desc\ of\ i}) \\
&= \prod_i^n P(Y_i|Y_{non-desc\ of\ i-\pi_i},Y_{\pi_i}), \\
&\ \ \ \ \ \ \ \ \ \ \ \ by \ Y_{non-desc\ of\ i} = Y_{non-desc\ of\ i-\pi_i} \cup Y_{\pi_i},  \\
&= \prod_i^n P(Y_i|Y_{\pi_i}), \ by\ X_i \perp X_{non-desc of i}|X_{\pi i}, \\
&= \prod_i^n P(X_i|X_{\pi_i}) \\
\end{align}


%----------------------------------------------------------------------------------------
%	PROBLEM 3
%----------------------------------------------------------------------------------------

\section{Problem 3}
Using the given graphical model we would like to find Walle.  
Let's create a junction tree based on the given graphical model of Walle.
Each step in creation of the graphical model will be represented by a figure describing the work done. 

The first step is moralization, which is completed by attaching nodes that both have the same child.  This can be seen in figure ~\ref{fig:moralization}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Problem3/Slide1.png}
\caption{Moralization of the directed graph into an undirected graph}
\label{fig:moralization}
\end{figure}

We then triangulate the graph by placing a connection between node B2 and B4, as shown in figure ~\ref{fig:trig}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Problem3/Slide2.png}
\caption{Triangulated graph of Wall-E}
\label{fig:trig}
\end{figure}

After this we then find the maximal cliques over the triangulated graphs, and the cliques can be seen in figure ~\ref{fig:clique}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Problem3/Slide3.png}
\caption{Maximal Cliques over the graph of Wall-E}
\label{fig:clique}
\end{figure}

Once we have the maximal cliques use Kruskal's algorithm in two stages to connect up the Junction Tree.  
We begin by connecting the nodes with seperators with the highest cardinality, which happen to be 2.
These seperators and there connections can be seen in figure ~\ref{fig:ver1}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Problem3/Slide4.png}
\caption{Nodes connected in Kruskal's algorithm with most seperator cardinality}
\label{fig:ver1}
\end{figure}

Then we connect the nodes using Kruskal's algorithm with seperator cardinality of 1, as long as the connection does not create a cycle, working down from the connections that are already present. 
The results of this can be seen in figure ~\ref{fig:ver2}, and the root node is the node containing elements B3,B2,B4.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Problem3/Slide5.png}
\caption{Final junction tree created using Kruskal's algorithm}
\label{fig:ver2}
\end{figure}

%----------------------------------------------------------------------------------------
%	PROBLEM 4
%----------------------------------------------------------------------------------------

\section{Problem 4}
In this problem we will analyze how the Junction Tree Algorithm is applied to a markov chain.
I have created a matlab script that allows the user to choose model the probabilities within a markov chain and generate the proper marginal distributions across the variables given the Junction Tree Algorithm.
This file was submitted with this report and is titled ``RunProblem4.m''.
The code has also been submitted at the end of this report.

To begin the process of solving the proper probability distributions over the variables we create a proper junction tree using the technique discussed above in Problem 3.
The junction tree nodes are quite simple, and given a chain of nodes $(x_1,x_2,...,x_n)$ we have $n-1$ nodes of which each are denoted as $x_{i}x_{i+1}$.
We then have n-2 seperators, which are the values of $x$ that are adjacent to two consecutive nodes.

Once this tree is created we then perform message passing as described in Jordan from left to right along the junction tree, and then right to left.
After this step we normalize each marginal so that we have proper marginals within the graphical model.
For the first message step we use the process described in Lecture 17 on slide 18, and where the seperator function is represented as $\phi_i$ and the function on the nodes is represented as $\psi_{i,i+1}$, where each are functions of some $x$ variables.
The update equations therefore can be seen below,

\begin{align}
\phi_i^* &= \sum_{x_{i-1}} \psi_{i-1,i} \\
\psi_{i,i+1}^* &= \frac{\phi_i^*}{\phi_i}\psi_{i,i+1}.
\end{align}

Once the messages have been passed from left to right, we then have ``starred'' values for each of the nodes and seperators, and then pass the message back from right to left according to the junction tree algorithm.
The corresponding right to left updates are described below, where are variable $i$ gets smaller with each update.

\begin{align}
\phi_i^{**} &= \sum_{x_{i}+1} \psi_{i,i+1}^{*} \\
\psi_{i-1,i}^{**} &= \frac{\phi_i^{**}}{\phi_i^*}\psi_{i-1,i}^*.
\end{align}

Now all of the $\psi$ and  $\phi$ functions are consistent with one another, but to create real joint distribution marginals ($\psi^{**}_{i,i+1} = p(x_i,x_{i+1}))$) each must be normalized.
To do this we simply divide each value in the $\psi$ tables by the sum of all of the entries that $\psi$ table.
Thus, we have proper marginal distributions on these variables.

To check that our algorithm is performing this calculation correctly we initalized the $\psi$ tables as described in Problem 4.
The final output of our algorithm can be seen below, where each $\psi^{**}$ has been normalized so it is a proper marginal distribution on the variables.

\begin{align}
p(x_1,x_2) &=
	\begin{pmatrix}
	0.0405 & 0.4451 \\
	0.3237 & 0.1908
	\end{pmatrix} \\
p(x_2,x_3) &=
	\begin{pmatrix}
	0.2601 & 0.1040 \\
    	0.0578 & 0.5780
	\end{pmatrix} \\
p(x_3,x_4) &=
	\begin{pmatrix}
	0.1192  &  0.1987 \\
    	0.6395   & 0.0426
	\end{pmatrix} \\
p(x_4,x_5) &=
	\begin{pmatrix}
	0.5690  &  0.1897 \\
    	0.0603  &  0.1810
	\end{pmatrix}
\end{align}

Now that we have the proper joint marginal distributions, let's check to make sure that each one is consistent.
We do this by summing each adjacent $\psi$ table over the non-seperator variables and make sure that they are the same.

\begin{align}
\sum_{x_1} p(x_1,x_2) &= (0.0405+0.3237,0.4451+0.1908) \\
&= (.3642,0.6359) = (0.2601+0.1040,0.0578+0.5780) \\ 
&= \sum_{x_3} p(x_2,x_3),
\end{align}

\begin{align}
\sum_{x_2} p(x_2,x_3) &= (0.2601+0.0578,0.1040+0.5780) \\
&= (.3179,0.6820) = (0.1192+0.1987,0.6395+0.0426) \\ 
&= \sum_{x_4} p(x_3,x_4),
\end{align}

\begin{align}
\sum_{x_3} p(x_3,x_4) &= (0.1192+0.6395,0.1987+0.0426) \\
&= (0.7587,0.2413) = (0.5690+0.1897,0.0603+0.1810) \\ 
&= \sum_{x_5} p(x_4,x_5).
\end{align}

Since the summed marginal distributions agree then we can see here that our marginals are consistent, and our algorithm has worked properly.
The script used to create these results can be seen in figure \ref{fig:script}.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{Problem3/Problem4_script.png}
\caption{Script Used to process juntion tree on markov chains}
\label{fig:script}
\end{figure}

\end{document}